<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Orange Sun - automation</title><link href="https://potyarkin.ml/" rel="alternate"></link><link href="https://potyarkin.ml/feeds/automation.atom.xml" rel="self"></link><id>https://potyarkin.ml/</id><updated>2020-02-25T00:00:00+03:00</updated><subtitle>Unsorted ramblings, sometimes related to programming</subtitle><entry><title>Running Libvirt (KVM) in Cirrus CI</title><link href="https://potyarkin.ml/posts/2020/running-libvirt-kvm-in-cirrus-ci/" rel="alternate"></link><published>2020-02-25T00:00:00+03:00</published><updated>2020-02-25T00:00:00+03:00</updated><author><name>Vitaly Potyarkin</name></author><id>tag:potyarkin.ml,2020-02-25:/posts/2020/running-libvirt-kvm-in-cirrus-ci/</id><summary type="html">&lt;p&gt;Up until the middle of 2019 it was very unusual to even expect that any CI
service would allow nested virtualization. Those who required such
functionality had to maintain their own CI runners on their own
infrastructure. Things changed when Google Cloud introduced &lt;a href="https://cloud.google.com/compute/docs/instances/enable-nested-virtualization-vm-instances"&gt;nested
KVM&lt;/a&gt; support.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cirrus-ci.org/"&gt;Cirrus CI&lt;/a&gt; was probably …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Up until the middle of 2019 it was very unusual to even expect that any CI
service would allow nested virtualization. Those who required such
functionality had to maintain their own CI runners on their own
infrastructure. Things changed when Google Cloud introduced &lt;a href="https://cloud.google.com/compute/docs/instances/enable-nested-virtualization-vm-instances"&gt;nested
KVM&lt;/a&gt; support.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cirrus-ci.org/"&gt;Cirrus CI&lt;/a&gt; was probably the first CI service to &lt;a href="https://cirrus-ci.org/guide/linux/#kvm-enabled-privileged-containers"&gt;officially
support&lt;/a&gt; nested virtualization in free tier. There are reports
that Travis CI currently also provides such feature but no public announcement
has been made yet.&lt;/p&gt;
&lt;p&gt;It turns out I was the first person to try using Libvirt in Cirrus CI (I've
even hit a previously unknown &lt;a href="https://github.com/cirruslabs/cirrus-ci-docs/issues/564"&gt;bug&lt;/a&gt; which was promptly fixed by their staff).
Since the process has some subtle differences to the popular documented use
cases I've decided to describe it here.&lt;/p&gt;
&lt;h2 id="hypervisor-environment"&gt;&lt;a class="toclink" href="#hypervisor-environment"&gt;Hypervisor environment&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Cirrus CI uses Docker images as environment for their runners. It
significantly simplifies the setup and enables efficient caching between runs.&lt;/p&gt;
&lt;p&gt;Since popular Docker images do not include any hypervisor packages we need to
build our own image. I've decided to add the required packages to Debian base
image. The whole &lt;a href="https://gitlab.com/sio/server_common/-/blob/master/ansible/tests/Dockerfile.host-kvm"&gt;Dockerfile&lt;/a&gt; is essentially one &lt;code&gt;apt-get&lt;/code&gt; statement.&lt;/p&gt;
&lt;p&gt;Keep in mind that libvirt package in Debian drops root privileges when
launching &lt;code&gt;qemu-kvm&lt;/code&gt;. You'll either need to disable that in
&lt;code&gt;/etc/libvirt/qemu.conf&lt;/code&gt; (as I did) or to change permissions for &lt;code&gt;/dev/kvm&lt;/code&gt; to
allow access by &lt;code&gt;libvirt-qemu&lt;/code&gt; user.&lt;/p&gt;
&lt;h2 id="required-system-services"&gt;&lt;a class="toclink" href="#required-system-services"&gt;Required system services&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Default entry point for CI runner is not customizable in Cirrus CI - it's an
agent process that communicates with CI service and sends progress reports you
see in web interface. Because of that no systemd units are started automatically
as it would have been the case on a normal system. More than that, starting
systemd manually also looks impossible.&lt;/p&gt;
&lt;p&gt;That means all the daemons required by libvirt must be started manually (see
documentation on &lt;a href="https://cirrus-ci.org/guide/writing-tasks/#background-script-instruction"&gt;background_script&lt;/a&gt; syntax):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# .cirrus.yml&lt;/span&gt;
&lt;span class="nt"&gt;dbus_background_script&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;mkdir -p /var/run/dbus&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/usr/bin/dbus-daemon --system --nofork --nopidfile&lt;/span&gt;
&lt;span class="nt"&gt;virtlogd_background_script&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/usr/sbin/virtlogd&lt;/span&gt;
&lt;span class="nt"&gt;libvirtd_background_script&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;sleep 2 &amp;amp;&amp;amp; /usr/sbin/libvirtd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id="firewall-configuration"&gt;&lt;a class="toclink" href="#firewall-configuration"&gt;Firewall configuration&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Hypervisor kernel is provided as is, and it currently runs legacy iptables
firewall. Trying to use iptables-nft (which is the default in current Debian)
produces a misconfigured guest network that is hard to debug.&lt;/p&gt;
&lt;p&gt;That's why we need to tell Debian to use legacy iptables interface across the whole
system:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# .cirrus.yml&lt;/span&gt;
&lt;span class="nt"&gt;iptables_legacy_script&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;update-alternatives --set iptables /usr/sbin/iptables-legacy&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id="_1"&gt;&lt;a class="toclink" href="#_1"&gt;&amp;nbsp;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;That's it! Following these steps I was able to execute Libvirt (via
Vagrant-Libvirt, via Molecule) in Cirrus CI environment. &lt;a href="https://gitlab.com/sio/server_common/-/blob/master/.cirrus.yml.j2"&gt;Full configuration&lt;/a&gt;
is available here, it includes some extra caching steps and many debug
statements that helped me to implement this process in the first place.&lt;/p&gt;</content><category term="posts"></category><category term="ci"></category><category term="automation"></category></entry><entry><title>Cirrus CI integration for GitLab projects</title><link href="https://potyarkin.ml/posts/2020/cirrus-ci-integration-for-gitlab-projects/" rel="alternate"></link><published>2020-02-18T00:00:00+03:00</published><updated>2020-02-18T00:00:00+03:00</updated><author><name>Vitaly Potyarkin</name></author><id>tag:potyarkin.ml,2020-02-18:/posts/2020/cirrus-ci-integration-for-gitlab-projects/</id><summary type="html">&lt;p&gt;&lt;a href="https://cirrus-ci.org/"&gt;Cirrus CI&lt;/a&gt; is a relatively new hosted CI service that offers several unique
features. It's probably the only CI provider to offer full virtualization
(KVM) or FreeBSD runners for free. Currently their business model is centered
around GitHub Marketplace and only the projects hosted at GitHub are
supported.&lt;/p&gt;
&lt;p&gt;Fortunately Cirrus …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://cirrus-ci.org/"&gt;Cirrus CI&lt;/a&gt; is a relatively new hosted CI service that offers several unique
features. It's probably the only CI provider to offer full virtualization
(KVM) or FreeBSD runners for free. Currently their business model is centered
around GitHub Marketplace and only the projects hosted at GitHub are
supported.&lt;/p&gt;
&lt;p&gt;Fortunately Cirrus CI provides a very capable GraphQL &lt;a href="https://cirrus-ci.org/api/"&gt;API&lt;/a&gt;. Thanks to that I
was able to write a simple command line tool to trigger CI builds with custom
configuration: &lt;a href="https://github.com/sio/cirrus-run"&gt;cirrus-run&lt;/a&gt;. It reads a local YAML file and executes Cirrus
CI build with that configuration. You can execute the build against any
GitHub repo you're allowed to access.&lt;/p&gt;
&lt;p&gt;Since build configuration is not provided by the repo the job may have no
relation to it. You can execute any number of jobs for any number of projects
against a single dummy holding repo at GitHub - which is the approach I
suggest to use for setting up integration with other source code hosting
platforms.&lt;/p&gt;
&lt;p&gt;Here is how I've set it up with GitLab:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Project repo is hosted at GitLab&lt;/li&gt;
&lt;li&gt;Each push triggers a &lt;a href="https://gitlab.com/sio/server_common/-/jobs/441624574"&gt;new build&lt;/a&gt; in GitLab CI.
    The only purpose of that build is to execute &lt;code&gt;cirrus-run&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/sio/cirrus-run"&gt;cirrus-run&lt;/a&gt; triggers the &lt;a href="https://cirrus-ci.com/task/5420732842770432"&gt;real build&lt;/a&gt; in Cirrus CI, waits for it to
    complete and reports the results.&lt;/p&gt;
&lt;p&gt;Cirrus CI build is owned by a dummy GitHub repo that contains only one
initial commit. Providing &lt;a href="https://cirrus-ci.org/guide/tips-and-tricks/#custom-clone-command"&gt;custom clone script&lt;/a&gt; allows to skip cloning
that dummy repo.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That allows me to continue using GitLab to host my project and GitLab CI to
run the jobs it's good at while delegating the more demanding jobs to Cirrus
CI. All status reports are gathered by GitLab CI and failure notifications
arrive uniformly to my inbox regardless of where the build was executed.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gitlab.com/sio/server_common/pipelines/118955114"&gt;&lt;img alt="Cirrus jobs in GitLab CI" src="https://potyarkin.ml/posts/2020/cirrus-ci-integration-for-gitlab-projects/cirrus-gitlab.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Configuration files that enable the workflow described above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gitlab.com/sio/server_common/-/blob/795a204b90ddfd95e36d2753d9c7ea6d3a9f6573/.gitlab-ci.yml#L46-55"&gt;GitLab CI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gitlab.com/sio/server_common/-/blob/795a204b90ddfd95e36d2753d9c7ea6d3a9f6573/.cirrus.yml.j2"&gt;Cirrus CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="posts"></category><category term="automation"></category><category term="ci"></category></entry><entry><title>Cygwin CI journey</title><link href="https://potyarkin.ml/posts/2020/cygwin-ci-journey/" rel="alternate"></link><published>2020-01-28T00:00:00+03:00</published><updated>2020-01-28T00:00:00+03:00</updated><author><name>Vitaly Potyarkin</name></author><id>tag:potyarkin.ml,2020-01-28:/posts/2020/cygwin-ci-journey/</id><summary type="html">&lt;p&gt;Setting up Cygwin CI environment for testing one of my projects took more
than fifty trial-and-error attempts - that's why I think it will be useful to
leave some written notes on the issues I've encountered. Here is the end
&lt;a href="https://github.com/sio/Makefile.venv/blob/master/.github/workflows/test.yml"&gt;result&lt;/a&gt; - GitHub CI running some Makefile tests in Cygwin.&lt;/p&gt;
&lt;h2 id="cygwin-gotchas"&gt;&lt;a class="toclink" href="#cygwin-gotchas"&gt;Cygwin gotchas …&lt;/a&gt;&lt;/h2&gt;</summary><content type="html">&lt;p&gt;Setting up Cygwin CI environment for testing one of my projects took more
than fifty trial-and-error attempts - that's why I think it will be useful to
leave some written notes on the issues I've encountered. Here is the end
&lt;a href="https://github.com/sio/Makefile.venv/blob/master/.github/workflows/test.yml"&gt;result&lt;/a&gt; - GitHub CI running some Makefile tests in Cygwin.&lt;/p&gt;
&lt;h2 id="cygwin-gotchas"&gt;&lt;a class="toclink" href="#cygwin-gotchas"&gt;Cygwin gotchas&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Cygwin installer likes to fail silently, providing only cryptic exit codes
    (&lt;code&gt;127&lt;/code&gt; or &lt;code&gt;-1073741571&lt;/code&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;%CYGWIN_ROOT%\setup.exe --quiet-mode --verbose --no-desktop --download --local-install --no-verify --site %CYGWIN_MIRROR% --local-package-dir %CYGWIN_PACKAGE_CACHE% --root %CYGWIN_ROOT%
Starting cygwin install, version 2.897
User has backup/restore rights
io_stream_cygfile: fopen(/etc/setup/setup.rc) failed 2 No such file or directory
Current Directory: cache\cygwin-packages
Could not open service McShield for query, start and stop. McAfee may not be installed, or we don&amp;#39;t have access.
root: d:\a\Makefile.venv\Makefile.venv\cache\cygwin system
Selected local directory: cache\cygwin-packages
##[error]Process completed with exit code -1073741571.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;At least some of those errors can be avoided by providing full absolute
paths to the installer and any argument values instead of relative
ones.&lt;/p&gt;
&lt;p&gt;In the end I've switched to Chocolatey to handle the installation for me.
It's certainly better but still requires calling &lt;code&gt;setup.exe&lt;/code&gt; to install
custom packages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since Cygwin installation and configuration are usually pretty slow (~3
    min), CI setup benefits a lot from caching this step between runs.
    Unfortunately, default GitHub actions for caching can't help with Cygwin
    root, because Windows is pretty hostile to an average Joe trying to make
    symlinks.&lt;/p&gt;
&lt;p&gt;Instead, I've added a separate step that calls &lt;code&gt;tar --dereference&lt;/code&gt;
explicitly and leaves only a simple archive for the caching action to
handle. Please note that tar uses exit code &lt;code&gt;1&lt;/code&gt; to indicate that
everything was OK, but some warnings were printed to stderr. Also, order
of command line arguments matters, at least for &lt;code&gt;--exclude&lt;/code&gt; values. That
was pretty unexpected for me.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You need to be careful when invoking apps under Cygwin - some environment
    variables may be inherited from &lt;code&gt;cmd&lt;/code&gt; session. For example, &lt;code&gt;$PATH&lt;/code&gt;
    will almost certainly point to binaries outside Cygwin root. I found that
    explicitly defining &lt;code&gt;$PATH&lt;/code&gt; simplifies matters a lot.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="powershellcmd-peculiarities"&gt;&lt;a class="toclink" href="#powershellcmd-peculiarities"&gt;PowerShell/cmd peculiarities&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Since host OS is still Windows, you get to encounter all the usual
PowerShell/cmd quirks. I tripped over the fact that PowerShell does not like
double-dashed --GNU-style options, and over weird nested quotes required to
properly combine environment variables and special characters in one value in
cmd.&lt;/p&gt;
&lt;h2 id="github-ci"&gt;&lt;a class="toclink" href="#github-ci"&gt;GitHub CI&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Some of my attempts were just me trying to work around GitHub CI limitations.
For example, dynamically calculating the value of environment variable based
on values of other variables. This looks quite ugly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Use absolute path for CYGWIN_ROOT&lt;/span&gt;
  &lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;echo &amp;quot;::set-env name=CYGWIN_ROOT::${env:GITHUB_WORKSPACE}\${env:CYGWIN_ROOT}&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I've also failed at proper syntax for YAML multiline strings more times than
I'm comfortable admitting :)&lt;/p&gt;</content><category term="posts"></category><category term="windows"></category><category term="cygwin"></category><category term="automation"></category><category term="ci"></category></entry><entry><title>Don't blindly trust Docker for the selfhosted stuff</title><link href="https://potyarkin.ml/posts/2020/no-docker-for-selfhosted/" rel="alternate"></link><published>2020-01-27T00:00:00+03:00</published><updated>2020-01-27T00:00:00+03:00</updated><author><name>Vitaly Potyarkin</name></author><id>tag:potyarkin.ml,2020-01-27:/posts/2020/no-docker-for-selfhosted/</id><summary type="html">&lt;p&gt;It is my strong belief that you shouldn't go crazy with &lt;em&gt;all-things-docker&lt;/em&gt;
when deploying selfhosted services at home. Online forums, especially
&lt;a href="https://reddit.com/r/selfhosted/"&gt;r/selfhosted&lt;/a&gt;, seem to foster an opinion that providing a &lt;code&gt;Dockerfile&lt;/code&gt; or
better yet a &lt;code&gt;docker-compose.yml&lt;/code&gt; or even prebuilt public images on Docker Hub
is an acceptable way …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It is my strong belief that you shouldn't go crazy with &lt;em&gt;all-things-docker&lt;/em&gt;
when deploying selfhosted services at home. Online forums, especially
&lt;a href="https://reddit.com/r/selfhosted/"&gt;r/selfhosted&lt;/a&gt;, seem to foster an opinion that providing a &lt;code&gt;Dockerfile&lt;/code&gt; or
better yet a &lt;code&gt;docker-compose.yml&lt;/code&gt; or even prebuilt public images on Docker Hub
is an acceptable way to distribute software targeting the selfhosting crowd.&lt;/p&gt;
&lt;p&gt;I agree it is very convenient to deploy complex multipart services via these
tools. But the way many people appear to be doing that is a &lt;em&gt;security
nightmare&lt;/em&gt;! This is how we get to encounter &lt;a href="https://www.computerweekly.com/news/252437100/Heartbleed-and-WannaCry-thriving-in-Docker-community"&gt;Heartbleed in the
wild&lt;/a&gt; four years after it should've been extinct.&lt;/p&gt;
&lt;p&gt;There are &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#protecting-cluster-components-from-compromise"&gt;many&lt;/a&gt; comprehensive &lt;a href="https://www.stackrox.com/post/2019/07/kubernetes-security-101/"&gt;writeups&lt;/a&gt; on
Docker/Kubernetes security, I will highlight only a subset of problems below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Shared libraries&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Running each service in its separate container results in having a
separate set of shared libraries for each one of those services. It is
convenient when you need to provide multiple incompatible dependencies at
once, but that way the burden of tracking the state of all those
dependencies lies on the user. Host OS can not tell you that one of the
containers still ships a vulnerable version of some critical library -
it's up to you to monitor and fix that.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Container rebuilding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Fixing anything related to the container requires rebuilding the image.
When you're using images from public registries you can not initiate image
rebuild even when you know it's needed. Your best option is to contact the
original uploader and to convince them to rebuild. That may take
significant time during which the containers running that image remain
vulnerable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Images from untrusted sources&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In addition to the points above you put enormous amount of trust into
people who provide the container you're running. In containerless scenario
you're required to trust the vendor who provides the base OS and the
developer who provides the custom applications you run upon that OS. When
containers come into play, you must extend your trust to the maintainer of
the container image, to the vendors who provide the base image that image
is built upon, to all the developers who provide any piece of code
included into that container. It does not even require malicious intent to
introduce a vulnerability into the resulting container, simple
incompetence of any of the parties involved may be just enough.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is why containerizing any workload comes with a significant extra cost of
designing and automating security maintenance procedures. It is easy to skip
this step when you're a hobbyist - but that's just burying your head in the
sand and waiting for some script kiddie or botnet to hijack your network.&lt;/p&gt;
&lt;p&gt;Here is a rough overview of the required overhead:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You need to run only containers that are based on the images you've built
  yourself. This is the only way you can ensure swift rebuilding in case one
  of the base images provides a security update. This step may include running
  your own image registry and build service.&lt;/li&gt;
&lt;li&gt;You need to audit every Dockerfile you intend to build. This can only be
  done manually. And you need to check all the base images in the chain up to
  either a &lt;code&gt;FROM scratch&lt;/code&gt; stanza or to a base image from trusted list.&lt;/li&gt;
&lt;li&gt;You need to maintain a list of trusted base images that come from vendors
  with good reputation in regards to handling security issues.&lt;/li&gt;
&lt;li&gt;You need to blacklist any image that does not come either from a trusted
  list or from a Dockerfile you've audited yourself.&lt;/li&gt;
&lt;li&gt;You need to setup automated image rebuilds and container rollouts:
    a) on schedule
    b) on any update in the base image dependency chain&lt;/li&gt;
&lt;li&gt;You need to setup automated vulnerability monitoring for the images you're
  running. This will require a lot more effort than subscribing to RSS feed of
  your distro security announcements - as it would've been the case with
  containerless deployment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Add that on top of usual container orchestration chores - and bare metal
suddenly becomes attractive. Docker and Kubernetes are great tools that solve
real world problems but using them in a secure manner requires continuous
dedicated effort. For enterprise deployments the benefits of containerization
usually outweigh the extra maintenance cost, but for hobbyist use I'm not so
sure.&lt;/p&gt;</content><category term="posts"></category><category term="docker"></category><category term="kubernetes"></category><category term="automation"></category></entry><entry><title>Manage Python virtual environment from your Makefile</title><link href="https://potyarkin.ml/posts/2019/manage-python-virtual-environment-from-your-makefile/" rel="alternate"></link><published>2019-10-01T00:00:00+03:00</published><updated>2019-10-01T00:00:00+03:00</updated><author><name>Vitaly Potyarkin</name></author><id>tag:potyarkin.ml,2019-10-01:/posts/2019/manage-python-virtual-environment-from-your-makefile/</id><summary type="html">&lt;p&gt;I often use Makefiles not just as a build tool but as a handy way to execute
sequences of commands. The commands I've found myself executing again and
again lately are the ones to manage Python virtual environments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create new venv&lt;/li&gt;
&lt;li&gt;Update pip to the latest version that enables all …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;I often use Makefiles not just as a build tool but as a handy way to execute
sequences of commands. The commands I've found myself executing again and
again lately are the ones to manage Python virtual environments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create new venv&lt;/li&gt;
&lt;li&gt;Update pip to the latest version that enables all the cool new features&lt;/li&gt;
&lt;li&gt;Install project requirements&lt;/li&gt;
&lt;li&gt;Delete venv and redo it again to see if everything still works from clean slate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The process is tedious and begs to be automated. And Makefile is a good fit
because in addition to basic scripting capabilities it offers proper
dependency handling that simplifies the task quite a bit.&lt;/p&gt;
&lt;p&gt;The outcome of my attempts at such automation is &lt;a href="https://github.com/sio/Makefile.venv"&gt;Makefile.venv&lt;/a&gt; - a Makefile
that seamlessly handles all virtual environment routines without ever needing
to be explicitly invoked. Instead, you write make targets that depend on
&lt;code&gt;venv&lt;/code&gt; and refer to all executables in virtual environment via
&lt;code&gt;$(VENV)/executable&lt;/code&gt;, e.g. &lt;code&gt;$(VENV)/python&lt;/code&gt; or &lt;code&gt;$(VENV)/pip&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Using &lt;a href="https://github.com/sio/Makefile.venv"&gt;Makefile.venv&lt;/a&gt; is easy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;.PHONY&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;
&lt;span class="nf"&gt;test&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt;
    &lt;span class="k"&gt;$(&lt;/span&gt;VENV&lt;span class="k"&gt;)&lt;/span&gt;/python -m unittest

&lt;span class="cp"&gt;include Makefile.venv  # All the magic happens here&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Despite its apparent simplicity this Makefile will do very much when invoked
(watch a &lt;a href="https://asciinema.org/a/279646"&gt;screencast&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A virtual environment will be created in current directory&lt;/li&gt;
&lt;li&gt;Pip will be automatically updated to the latest version&lt;/li&gt;
&lt;li&gt;Project requirements will be installed (both &lt;code&gt;requirements.txt&lt;/code&gt; and
  &lt;code&gt;setup.py&lt;/code&gt; are supported)&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;setup.py&lt;/code&gt; is present, the project will be installed in development mode
  into venv (&lt;code&gt;pip install -e&lt;/code&gt;) - all changes to the source code will
  immediately affect the package in virtual environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these steps will be repeated in case &lt;code&gt;requirements.txt&lt;/code&gt; or &lt;code&gt;setup.py&lt;/code&gt;
is modified. That means you'll never have to worry about syncing venv with its
description. Add new dependency to &lt;code&gt;setup.py&lt;/code&gt; and consider it installed,
because there is no way it'll be forgotten the next time you invoke &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you'll need to debug something interactively, there are &lt;code&gt;make python&lt;/code&gt; and
&lt;code&gt;make ipython&lt;/code&gt; for REPL and &lt;code&gt;make bash&lt;/code&gt; (or &lt;code&gt;shell&lt;/code&gt; or &lt;code&gt;zsh&lt;/code&gt;) for shell, but I
rarely use those. Most of the time running &lt;code&gt;make&lt;/code&gt; with my targets for
executing the entry point or running unit tests is enough. In fact, I've
noticed that after introducing &lt;a href="https://github.com/sio/Makefile.venv"&gt;Makefile.venv&lt;/a&gt; into my workflow I've
completely stopped activating virtual environments manually.&lt;/p&gt;
&lt;p&gt;I encourage you to try &lt;a href="https://github.com/sio/Makefile.venv"&gt;Makefile.venv&lt;/a&gt; and hope you'll find this approach
useful. If you have some comments or would like to point out the faults of
using Makefiles for venv, please shoot me an e-mail or create an &lt;a href="https://github.com/sio/Makefile.venv/issue"&gt;issue&lt;/a&gt; at
GitHub project's page.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;PS: &lt;a href="https://github.com/sio/Makefile.venv"&gt;Makefile.venv&lt;/a&gt; was inspired by &lt;a href="https://stackoverflow.com/questions/24736146"&gt;this StackOverflow
thread&lt;/a&gt; and by &lt;a href="http://blog.bottlepy.org/2012/07/16/virtualenv-and-makefiles.html"&gt;this blog post&lt;/a&gt; from the authors of
Bottle.py&lt;/p&gt;
&lt;/blockquote&gt;</content><category term="posts"></category><category term="python"></category><category term="automation"></category></entry><entry><title>On dotfiles management</title><link href="https://potyarkin.ml/posts/2019/on-dotfiles-management/" rel="alternate"></link><published>2019-07-30T00:00:00+03:00</published><updated>2019-07-30T00:00:00+03:00</updated><author><name>Vitaly Potyarkin</name></author><id>tag:potyarkin.ml,2019-07-30:/posts/2019/on-dotfiles-management/</id><summary type="html">&lt;p&gt;This will be yet another description of dotfiles management by some random
person on the Internet. I will try to explain what my setup is like and why it
is that way.&lt;/p&gt;
&lt;p&gt;If you're not yet using version control software for your configuration files
I strongly encourage you to start …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This will be yet another description of dotfiles management by some random
person on the Internet. I will try to explain what my setup is like and why it
is that way.&lt;/p&gt;
&lt;p&gt;If you're not yet using version control software for your configuration files
I strongly encourage you to start doing so, whichever way you like. These
pages are good places to start:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://dotfiles.github.io/"&gt;An unofficial guide to dotfiles on GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wiki.archlinux.org/index.php/Dotfiles"&gt;Arch wiki article on dotfiles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="my-chosen-approach"&gt;&lt;a class="toclink" href="#my-chosen-approach"&gt;My chosen approach&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;After several attempts that have spanned across many years I've understood
that neither tracking home directory directly with Git nor symlinking all of
the dotfiles from a single directory tree is working for me. Both ways lead to
a mess in the repository and made the whole endeavor of tracking changes
cognitively expensive, so I inevitably started slacking off.&lt;/p&gt;
&lt;p&gt;I've looked at the existing tools that are meant to automate some of the
process and did not find one that would suit all my needs. I've ended up
writing a small shell &lt;a href="https://gitlab.com/sio/server_common/blob/master/dotfiles/bootstrap.sh"&gt;script&lt;/a&gt; that takes care of dotfiles installation but
the main value for me is in the repo layout, not the script itself.&lt;/p&gt;
&lt;p&gt;All configuration files are grouped into directories by topic. These
directories are somewhat similar to packages in GNU &lt;a href="https://www.gnu.org/software/stow/"&gt;stow&lt;/a&gt;. Topic directories
recreate the directory structure for the target location, by default $HOME.
Files that are meant to be installed into target location have to contain an
appropriate suffix at the end of filename (any other files are ignored):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.copy&lt;/code&gt; - for files to be copied over to new location&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.link&lt;/code&gt; - for files to be linked to from new location&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.append&lt;/code&gt; - for files to be appended to the target file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Default behavior may be altered by a &lt;code&gt;dotfiles.meta&lt;/code&gt; file placed into the
topic directory. It is essentially a shell script that is being sourced during
topic installation. Its main purpose is to provide alternative values for
PREFIX and SCOPE variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PREFIX value determines target directory where the dotfiles will be placed.
  Also if PREFIX is set the dotfiles will not get an extra dot in front of
  their filename (which is the default behavior otherwise).&lt;/li&gt;
&lt;li&gt;SCOPE variable may be used to indicate that a topic requires root privileges
  to be installed (&lt;code&gt;SCOPE=system&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Multiple topics may be installed at once either by providing all of their
names as command line arguments or by listing them all in a text file and
providing path to that file as an argument to the installation &lt;a href="https://gitlab.com/sio/server_common/blob/master/dotfiles/bootstrap.sh"&gt;script&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="examples"&gt;&lt;a class="toclink" href="#examples"&gt;Examples&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;topic-foo/vimrc.link&lt;/code&gt; will be symlinked from &lt;code&gt;~/.vimrc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;topic-bar/bashrc.copy&lt;/code&gt; will be copied over to &lt;code&gt;~/.bashrc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;topic-baz/default/keyboard.copy&lt;/code&gt; with &lt;code&gt;PREFIX=/etc&lt;/code&gt; will be copied to &lt;code&gt;/etc/default/keyboard&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;topic-baz/file/without/valid/suffix&lt;/code&gt; will be ignored&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More examples may be found in my &lt;a href="https://gitlab.com/sio/server_common/tree/master/dotfiles"&gt;dotfiles repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="comparison-with-existing-tools"&gt;&lt;a class="toclink" href="#comparison-with-existing-tools"&gt;Comparison with existing tools&lt;/a&gt;&lt;/h2&gt;
&lt;h4 id="strengths"&gt;&lt;a class="toclink" href="#strengths"&gt;Strengths&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Very small number of dependencies makes this script usable across all my
  Linux and Windows machines. It requires only the core GNU userland: bash,
  coreutils, find and grep.&lt;/li&gt;
&lt;li&gt;Multiple install actions are supported (copy, link, append) unlike &lt;a href="https://www.gnu.org/software/stow/"&gt;stow&lt;/a&gt;
  that only makes symlinks. More than that, my script detects if it's being
  executed on Windows machine and copies over any file that was meant to be
  symlinked - because symlinks on Windows are so tricky they're might as well
  be not supported at all.&lt;/li&gt;
&lt;li&gt;Destination directory may be specified for each topic individually which
  makes it possible to install topics targeting different directories in one
  run.&lt;/li&gt;
&lt;li&gt;Simple partial deployment. If machine requires only a subset of topics
  tracked in the repository it is easy to list them all in a plain text file
  or to provide them as command line arguments to the bootstrap script.
  &lt;a href="https://thelocehiliosan.github.io/yadm/"&gt;yadm&lt;/a&gt;, for example does not provide such ability.&lt;/li&gt;
&lt;li&gt;Dotfiles are not hidden in the repo by default. It makes no sense to have
  &lt;code&gt;~/.bashrc&lt;/code&gt; point to &lt;code&gt;repo/bash/.bashrc&lt;/code&gt; instead of &lt;code&gt;repo/bash/bashrc&lt;/code&gt;, so
  dots are added automatically for topics with default target PREFIX.&lt;/li&gt;
&lt;li&gt;All operations are reversible because all overwritten files are backed up
  beforehand.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="weaknesses"&gt;&lt;a class="toclink" href="#weaknesses"&gt;Weaknesses&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Single pass execution. It means some topics may be left partially configured
  in case of errors. &lt;a href="https://www.gnu.org/software/stow/"&gt;stow&lt;/a&gt; is a good example of cautious approach. This is an
  implementation detail and may be fixed in later versions of bootstrap
  script.&lt;/li&gt;
&lt;li&gt;No support for tree folding/unfolding. I consider that an overkill for
  simple configuration management.&lt;/li&gt;
&lt;li&gt;No automated reverse operation. In case you want to undo the changes made by
  this &lt;a href="https://gitlab.com/sio/server_common/blob/master/dotfiles/bootstrap.sh"&gt;script&lt;/a&gt; you'll have to restore backups manually from &lt;code&gt;$DOTFILES_BACKUP&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="posts"></category><category term="linux"></category><category term="windows"></category><category term="automation"></category><category term="bash"></category></entry><entry><title>Liberating effect of Ansible</title><link href="https://potyarkin.ml/posts/2018/liberating-effect-of-ansible/" rel="alternate"></link><published>2018-06-26T00:00:00+03:00</published><updated>2018-06-26T00:00:00+03:00</updated><author><name>Vitaly Potyarkin</name></author><id>tag:potyarkin.ml,2018-06-26:/posts/2018/liberating-effect-of-ansible/</id><summary type="html">&lt;p&gt;Maintaining two or three Linux machines is not that hard of a task. For many
years I have thought it was not worth the effort to automate - regular backups
and version-controlled configuration files seemed to be just enough.&lt;/p&gt;
&lt;p&gt;And then Ansible had blown my mind.&lt;/p&gt;
&lt;h2 id="history"&gt;&lt;a class="toclink" href="#history"&gt;History&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It all started with …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Maintaining two or three Linux machines is not that hard of a task. For many
years I have thought it was not worth the effort to automate - regular backups
and version-controlled configuration files seemed to be just enough.&lt;/p&gt;
&lt;p&gt;And then Ansible had blown my mind.&lt;/p&gt;
&lt;h2 id="history"&gt;&lt;a class="toclink" href="#history"&gt;History&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It all started with a web server and a series of subpar hosting providers.
Setting that server up for the first time was an adventure. Repeating the setup
for the first relocation has given me a chance to introduce some improvements
but was otherwise uneventful. I started dreading the process when the need for
the third iteration had arisen. I was willing to put up with sluggishness and
several short downtimes just to delay the move to a new server. That was not OK.&lt;/p&gt;
&lt;p&gt;Automating has clearly become a worthwhile task. I chose Ansible because it
doesn't require any special software on the controlled machines and because it
is mature enough to remain backwards compatible after updates. There are
numerous reviews of pros and cons of different configuration management systems,
but that's outside the scope of this article.&lt;/p&gt;
&lt;h2 id="unexpected-outcome"&gt;&lt;a class="toclink" href="#unexpected-outcome"&gt;Unexpected outcome&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Ansible has definitely succeeded at the task I've thrown it at. That was
expected. What I couldn't foresee is how this experience would affect my
mindset - it was like a breath of fresh air! I have suddenly started to
understand why there exists all the buzz around &lt;em&gt;the cloud&lt;/em&gt; and why cloud
service providers are dominating the market of virtual servers.&lt;/p&gt;
&lt;h3 id="certainty"&gt;&lt;a class="toclink" href="#certainty"&gt;Certainty&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;With my Ansible playbook I did not just automate the setup process, I created an
enforceable specification of what my server has to be like. At any time in the
future after executing the same playbook I can be certain that all configurable
parameters will be at the values I've defined.&lt;/p&gt;
&lt;p&gt;Idempotent behavior allows me to run the playbook again and again without fear
of breaking anything. It should be noted though, that not all Ansible modules
are idempotent and one should always check the documentation before
incorporating a new module into the playbook.&lt;/p&gt;
&lt;h3 id="immutability"&gt;&lt;a class="toclink" href="#immutability"&gt;Immutability&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There is even an option to take it one step further to truly &lt;a href="https://www.digitalocean.com/community/tutorials/what-is-immutable-infrastructure"&gt;immutable
infrastructure&lt;/a&gt; where any server can die at any point and no harm will be done.
This approach is for a bigger scale than a humble hobby project, though.&lt;/p&gt;
&lt;p&gt;I have limited the immutability effort to a gentle reminder in /etc/motd and a
policy among administrators (me, myself and I) that no configuration changes are
to be made via shell connection.&lt;/p&gt;
&lt;h3 id="disposability"&gt;&lt;a class="toclink" href="#disposability"&gt;Disposability&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now it is incredibly easy for me to replicate that web server. It requires only
one command and a short coffee break. That means I can switch the hosting
provider at any moment I want. Choosing the hosting company has become a
non-issue: I invest only a minimal payment and no labor at all. If I don't like
what I get I'll be gone in no time.&lt;/p&gt;
&lt;p&gt;Easy replication also means I can fire up another server for testing purposes,
then destroy it in a couple of hours and it will cost me only a few cents
because of hourly billing that most providers offer nowadays.&lt;/p&gt;
&lt;h3 id="emotional-detachment"&gt;&lt;a class="toclink" href="#emotional-detachment"&gt;Emotional detachment&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Setting up a new server used to be somewhat similar to moving into a new
apartment.  First, there was scrupulous comparing of offers, after that a
dramatic moment of signing the lease and then the silent minutes alone in empty
apartment, staring at the walls. Each server was important and loved, breaking
or destroying it on purpose was unthinkable.&lt;/p&gt;
&lt;p&gt;Now it's more like checking into a room at the hotel. And you get to be a
celebrity and send the rider list in advance, so that everything is set up to
your liking when you arrive. You can also check out at any moment without
worrying about the lease. There is no need to lug your favorite vimrc along
because you won't be editing anything there, in fact you'll hardly ever spend
any time logged in at all.&lt;/p&gt;
&lt;p&gt;Randy Bias has summed this attitude in the &lt;a href="http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/"&gt;pets versus cattle&lt;/a&gt; analogy. I think
it's quite on point even when you're managing a single machine, not the fleet of
servers.&lt;/p&gt;
&lt;h3 id="infrastructure-as-code"&gt;&lt;a class="toclink" href="#infrastructure-as-code"&gt;Infrastructure as code&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;And last, but not least, all you do with Ansible is automatically documented in
the playbook. You can use any version control system to track when any
particular change was introduced and why. That allows to revert the unwanted
changes as easily as was introducing them in the first place.&lt;/p&gt;
&lt;h2 id="afterword"&gt;&lt;a class="toclink" href="#afterword"&gt;Afterword&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When you're outside the professional IT crowd it is not obvious that the cloud
infrastructure and the corresponding tools are within the reach of hobbyist
enthusiasts. But they are and they offer just as much value for personal
projects as they do in production environment.&lt;/p&gt;
&lt;p&gt;If you find yourself tinkering with Linux administration more than once a week
it'll be worthwhile to try out Ansible or some other configuration management
tool.&lt;/p&gt;</content><category term="posts"></category><category term="ansible"></category><category term="web"></category><category term="server"></category><category term="automation"></category></entry></feed>